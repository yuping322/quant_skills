# 字段统一化完整工作流设计

## 整体架构

```
┌─────────────────────────────────────────────────────────────────┐
│                         完整工作流程                               │
└─────────────────────────────────────────────────────────────────┘

┌───────────────┐     ┌───────────────┐     ┌───────────────┐
│   数据来源    │────▶│  分析与统计   │────▶│  生成Prompts  │
│ (data目录 +   │     │ (提取字段信息)│     │ (准备LLM分析) │
│  akshare接口) │     └───────────────┘     └───────────────┘
└───────────────┘                                 │
                                                  │
                                                  ▼
┌───────────────┐     ┌───────────────┐     ┌───────────────┐
│  应用配置     │◀────│  LLM分析生成  │◀────│  用户手动操作  │
│ (unified_    │     │  config.json  │     │ (使用Prompts)  │
│  config.json)│     └───────────────┘     └───────────────┘
└───────────────┘                                 │
                                                  │
                                                  ▼
┌───────────────┐     ┌───────────────┐
│ 统一字段系统  │────▶│ 实际接口调用   │
│ (使用配置文件)│     │ (自动转换字段) │
└───────────────┘     └───────────────┘
```

## 详细步骤

### Step 1: 数据收集与分析

**目标：** 从 `data/` 目录和 akshare 接口获取完整的字段信息

**执行：**
```bash
python src/scripts/llm_field_analyzer_v2.py
```

**输出：**
- `analysis/complete_field_statistics.json` - 完整的字段统计信息
- `prompts/enhanced_field_equivalence_analysis.txt` - 字段等价关系分析提示词
- `prompts/enhanced_field_coverage_analysis.txt` - 字段覆盖分析提示词

**包含信息：**
- 2936个不同字段
- 851个接口的使用情况
- 每个字段的使用频率
- 输入/输出参数统计
- 使用的接口列表
- 字段描述示例

---

### Step 2: LLM分析（用户手动执行）

**目标：** 使用大模型分析字段语义等价关系

**操作：**
1. 打开 `prompts/enhanced_field_equivalence_analysis.txt`
2. 将内容提交给大模型
3. 保存分析结果为 `config/llm_equivalence_analysis.json`

4. 打开 `prompts/enhanced_field_coverage_analysis.txt`
5. 将内容提交给大模型
6. 保存分析结果为 `config/llm_coverage_analysis.json`

**期望输出格式：**
```json
{
  "field_groups": [
    {
      "canonical": "date",
      "equivalents": ["日期", "DATE", "TRADE_DATE"],
      "reason": "都是表示日期的字段",
      "confidence": "high"
    }
  ]
}
```

---

### Step 3: 配置生成

**目标：** 将LLM分析结果转换为系统配置

**执行：**
```bash
python src/scripts/generate_unified_config.py
```

**输入：**
- `config/llm_equivalence_analysis.json`
- `config/llm_coverage_analysis.json`

**输出：**
- `config/unified_field_config.json` - 完整的统一字段配置

**配置文件结构：**
```json
{
  "version": "1.0",
  "generated_at": "2026-02-18",
  "field_equivalents": {
    "date": ["日期", "DATE", "TRADE_DATE"],
    "symbol": ["代码", "股票代码", "code"],
    ...
  },
  "api_mappings": {
    "stock_zh_a_hist": {
      "input": {
        "symbol": "symbol",
        "start_date": "start_date"
      },
      "output": {
        "日期": "date",
        "开盘": "open"
      }
    }
  },
  "value_normalization": {
    "date": "date_formatter",
    "symbol": "stock_code_formatter"
  }
}
```

---

### Step 4: 系统使用配置

**目标：** 修改 `unified_field_system.py` 从配置文件读取

**修改内容：**
- 不再硬编码 `field_equivalents`
- 从 `config/unified_field_config.json` 加载配置
- 支持配置热更新

**使用方式：**
```python
from unified_field_system import UnifiedFieldSystem

# 初始化系统（自动加载配置）
system = UnifiedFieldSystem(config_path="config/unified_field_config.json")

# 执行接口（自动处理字段转换）
result = system.execute_with_unification(
    api_name="stock_zh_a_hist",
    user_input={
        "代码": "000001",
        "开始日期": "2024-01-01"
    }
)
```

---

### Step 5: 验证与测试

**目标：** 确保配置正确工作

**执行：**
```bash
# 验证字段等价关系
python src/scripts/validate_field_equivalents.py

# 测试统一字段系统
python src/unified_field_system.py --test
```

**检查项：**
- 字段等价关系无逻辑错误
- 字段覆盖率符合预期
- 实际接口调用正常工作
- 输入输出字段正确转换

---

## 文件结构

```
quant_skills/
├── config/                           # 配置文件目录（新建）
│   ├── llm_equivalence_analysis.json  # LLM等价关系分析结果
│   ├── llm_coverage_analysis.json     # LLM覆盖分析结果
│   └── unified_field_config.json       # 最终的统一字段配置
├── data/                             # 原始接口数据
├── src/
│   ├── mappers/
│   │   └── field_mapper.py            # 字段映射器（改为从配置读取）
│   ├── scripts/
│   │   ├── llm_field_analyzer_v2.py   # LLM分析器（已存在）
│   │   ├── validate_field_equivalents.py # 验证工具（已存在）
│   │   └── generate_unified_config.py  # 配置生成器（新建）
│   └── unified_field_system.py        # 统一字段系统（修改）
├── prompts/                          # 提示词目录（已存在）
├── analysis/                         # 分析报告目录（已存在）
└── docs/                            # 文档目录
    └── 完整工作流设计.md              # 本文档
```

---

## 配置文件详细说明

### 1. field_equivalents

字段等价关系定义，标准字段为key，等价字段列表为value。

```json
{
  "field_equivalents": {
    "date": ["日期", "DATE", "TRADE_DATE", "交易日期"],
    "symbol": ["代码", "股票代码", "code", "stock_code"],
    "name": ["名称", "股票名称", "简称"],
    "close": ["收盘价", "CLOSE", "最新价"],
    "open": ["开盘价", "OPEN", "今开"],
    "high": ["最高价", "HIGH", "最高"],
    "low": ["最低价", "LOW", "最低"],
    "volume": ["成交量", "VOLUME"],
    "amount": ["成交额", "AMOUNT"]
  }
}
```

### 2. api_mappings

每个接口的具体字段映射。

```json
{
  "api_mappings": {
    "stock_zh_a_hist": {
      "input": {
        "symbol": "symbol",
        "period": "period",
        "start_date": "start_date",
        "end_date": "end_date",
        "adjust": "adjust"
      },
      "output": {
        "日期": "date",
        "开盘": "open",
        "收盘": "close",
        "最高": "high",
        "最低": "low",
        "成交量": "volume",
        "成交额": "amount"
      }
    }
  }
}
```

### 3. value_normalization

值标准化规则。

```json
{
  "value_normalization": {
    "date": {
      "formatter": "date",
      "format": "YYYY-MM-DD"
    },
    "symbol": {
      "formatter": "stock_code",
      "market_prefix": true
    },
    "numeric": {
      "formatter": "float",
      "decimal_places": 4
    }
  }
}
```

---

## 工作流脚本

### 一键启动分析

```bash
# 完整工作流启动脚本
./scripts/run_workflow.sh
```

这个脚本会：
1. 运行 LLM 分析器生成提示词
2. 提示用户进行 LLM 分析
3. 等待用户保存分析结果
4. 生成统一配置
5. 验证配置
6. 测试系统

---

## 迭代改进流程

```
┌─────────────────┐
│  初始分析       │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  生成配置       │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  测试验证       │
└────────┬────────┘
         │
         ├─ 发现问题 ──▶ 修正配置 ──┐
         │                            │
         ▼                            │
┌─────────────────┐                   │
│  实际使用       │◀──────────────────┘
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  收集反馈       │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  重新分析       │
└─────────────────┘
```

---

## 总结

这个工作流的优势：

1. **数据驱动** - 基于实际数据统计，不是主观猜测
2. **LLM增强** - 利用大模型的语义理解能力
3. **配置分离** - 配置与代码分离，易于维护
4. **可迭代** - 支持持续改进和优化
5. **可验证** - 完整的验证和测试流程
